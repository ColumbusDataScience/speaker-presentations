{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZOYTt1RW4TK"
   },
   "source": [
    "### Install dependencies\n",
    "\n",
    "> #### Note\n",
    "> Because of some of the updates to packages you **must** use the button at the bottom of the output of this cell to restart the runtime.  Following restart, you should rerun this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2KzNtyo5KGa"
   },
   "outputs": [],
   "source": [
    "!pip install -U tensorflow==2.0.0 tensorboard==2.0.1\n",
    "!pip freeze | grep -e tensorflow -e tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vF4Up24Qv0w"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import urllib\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2cMMAbSkGfX"
   },
   "source": [
    "# Download example data\n",
    "We download the sample dataset for use in our TFX pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiKokHMYQ2-k"
   },
   "outputs": [],
   "source": [
    "# Download the example data.\n",
    "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
    "DATA_PATH = (\n",
    "    'https://raw.githubusercontent.com/tensorflow/' +\n",
    "    'tfx/master/tfx/examples/chicago_taxi_pipeline/' +\n",
    "    'data/simple/data.csv')\n",
    "with open(os.path.join(_data_root, 'data.csv'), 'wb') as f:\n",
    "    contents = urllib.request.urlopen(DATA_PATH).read()\n",
    "    f.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uuS0hbaSics"
   },
   "source": [
    "# Build some convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4gduzBaRIKk"
   },
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, target_col, shuffle=True, batch_size=32):\n",
    "    '''Takes a Pandas dataframe as an input and converts it\n",
    "    to a Tensorflow dataset\n",
    "    '''\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(target_col)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def build_model_input_columns(data):\n",
    "    outputs = {}\n",
    "\n",
    "    for key in DENSE_FLOAT_FEATURE_KEYS:\n",
    "        outputs[key] = tf.feature_column.numeric_column(\n",
    "            key,\n",
    "            shape=()\n",
    "        )\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "        outputs[key] = tf.feature_column.categorical_column_with_identity(\n",
    "            key,\n",
    "            num_buckets=VOCAB_SIZE + OOV_SIZE,\n",
    "            default_value=0\n",
    "        )\n",
    "        outputs[key] = tf.feature_column.embedding_column(\n",
    "            outputs[key],\n",
    "            dimension=math.ceil((VOCAB_SIZE + OOV_SIZE)**0.25)\n",
    "        )\n",
    "    for key in VOCAB_FEATURE_KEYS:\n",
    "        _vocab = data[key].drop_duplicates().to_list()\n",
    "        outputs[key] = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key,\n",
    "            vocabulary_list=_vocab,\n",
    "            dtype=tf.string,\n",
    "            num_oov_buckets=OOV_SIZE\n",
    "        )\n",
    "        if len(_vocab) > 10:\n",
    "            outputs[key] = tf.feature_column.embedding_column(\n",
    "                outputs[key],\n",
    "                dimension=math.ceil((VOCAB_SIZE + OOV_SIZE)**0.25)\n",
    "            )\n",
    "        else:\n",
    "            outputs[key] = tf.feature_column.indicator_column(outputs[key])\n",
    "\n",
    "    for key in BUCKET_FEATURE_KEYS:\n",
    "        outputs[key] = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "            key,\n",
    "            hash_bucket_size=FEATURE_BUCKET_COUNT\n",
    "        )\n",
    "        outputs[key] = tf.feature_column.indicator_column(outputs[key])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnMbarh6a-zg"
   },
   "source": [
    "# Define column schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qbNfic1TvWk"
   },
   "outputs": [],
   "source": [
    "DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
    "    'pickup_community_area', 'dropoff_community_area']\n",
    "\n",
    "VOCAB_FEATURE_KEYS = ['payment_type', 'company',]\n",
    "\n",
    "# We will quantize a few continuous variables, namely pickup and dropoff\n",
    "# coordinates. Discretizing them just makes them marginally easier to deal\n",
    "# with.\n",
    "BUCKET_FEATURE_KEYS = [\n",
    "    'pickup_latitude', 'pickup_longitude',\n",
    "    'dropoff_latitude', 'dropoff_longitude']\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "OOV_SIZE=10\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "LABEL_KEY = 'tips'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNS_4sPIbF9U"
   },
   "source": [
    "# Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvywP0M3SwGI"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(_data_root + '/data.csv')\n",
    "data = data.drop(\n",
    "    axis=1,\n",
    "    columns=[\n",
    "        'pickup_census_tract',\n",
    "        'dropoff_census_tract',\n",
    "        'trip_start_timestamp'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__ogyRSwb9Tg"
   },
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtype in ['int64', 'float64']:\n",
    "        data[col] = data[col].fillna(0)\n",
    "    else:\n",
    "        data[col] = data[col].fillna('')\n",
    "\n",
    "data['tips'] = np.where(data['tips'] > data['fare'] * 0.2, 1, 0)\n",
    "data['dropoff_community_area'] = data['dropoff_community_area'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_S9tjwhdiYA"
   },
   "source": [
    "We'll split the data into train, test, and validation subsets, and then convert them into TensorFlow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdRwFbi1awkk"
   },
   "outputs": [],
   "source": [
    "# split into train and test set\n",
    "train, test = train_test_split(\n",
    "    data,\n",
    "    test_size=0.20,\n",
    "    random_state=42)\n",
    "\n",
    "train, val = train_test_split(\n",
    "    train,\n",
    "    test_size=0.20,\n",
    "    random_state=42)\n",
    "\n",
    "train = pd.concat([\n",
    "    train[train[LABEL_KEY] == 0],\n",
    "    resample(\n",
    "        train[train[LABEL_KEY] == 1],\n",
    "        replace=True,\n",
    "        n_samples=round(train[train[LABEL_KEY] == 1].shape[0] * 1.5),\n",
    "        random_state=42)],\n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-vxnzeCa40s"
   },
   "outputs": [],
   "source": [
    "_batch_size = 32\n",
    "\n",
    "train_ds = df_to_dataset(\n",
    "    train,\n",
    "    target_col=LABEL_KEY,\n",
    "    batch_size=_batch_size)\n",
    "val_ds = df_to_dataset(\n",
    "    val,\n",
    "    target_col=LABEL_KEY,\n",
    "    shuffle=False,\n",
    "    batch_size=_batch_size)\n",
    "test_ds = df_to_dataset(\n",
    "    test,\n",
    "    target_col=LABEL_KEY,\n",
    "    shuffle=False,\n",
    "    batch_size=_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5KuQ1dDpZjw"
   },
   "source": [
    "# Define model form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIWIf7XQUei4"
   },
   "outputs": [],
   "source": [
    "model_columns = build_model_input_columns(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Z63yeupdwcA"
   },
   "source": [
    "Things get mildly funky here, but basically it just boils down to the Keras API expects inputs of `tf.keras.layers.Input`, but we've got a boatload of `tf.feature_column`s. Here, we're just mapping from one to the other so everything plays nice and we get to use not-infuriating Keras APIs with the added bonus of feature vocabularies when we go to deploy things down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS14728lcst4"
   },
   "outputs": [],
   "source": [
    "deep_layer_inputs, deep_columns = {}, []\n",
    "wide_layer_inputs, wide_columns = {}, []\n",
    "\n",
    "for key, column in model_columns.items():\n",
    "    if key in DENSE_FLOAT_FEATURE_KEYS:\n",
    "        deep_layer_inputs[key] = tf.keras.Input(\n",
    "            shape=(),\n",
    "            name=key,\n",
    "            dtype=column._asdict().get('dtype', tf.string)\n",
    "        )\n",
    "        deep_columns.append(column)\n",
    "    else:\n",
    "        wide_layer_inputs[key] = tf.keras.Input(\n",
    "            shape=(),\n",
    "            name=key,\n",
    "            dtype=column._asdict().get(\n",
    "                'dtype',\n",
    "                tf.string if key in BUCKET_FEATURE_KEYS + VOCAB_FEATURE_KEYS \\\n",
    "                else tf.int64)\n",
    "        )\n",
    "        wide_columns.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anFSQvKVev_M"
   },
   "source": [
    "We'll use a kind of janky implementation of Google's reference Wide-and-Deep recommender system architecture described in [arXiv:1606](https://arxiv.org/abs/1606.07792). It's not an awesome implementation, but it illustrates things well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFZh9Lt0eNWQ"
   },
   "outputs": [],
   "source": [
    "deep_layer_outputs = \\\n",
    "    tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=deep_columns)(deep_layer_inputs)\n",
    "wide_layer_outputs = \\\n",
    "    tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=wide_columns)(wide_layer_inputs)\n",
    "\n",
    "deep = tf.keras.layers.Dense(\n",
    "    100,\n",
    "    activation='relu',\n",
    "    name='dense_1')(deep_layer_outputs)\n",
    "deep = tf.keras.layers.BatchNormalization(name='norm_1')(deep)\n",
    "\n",
    "deep = tf.keras.layers.Dense(\n",
    "    75,\n",
    "    activation='relu',\n",
    "    name='dense_2')(deep)\n",
    "deep = tf.keras.layers.BatchNormalization(name='norm_2')(deep)\n",
    "\n",
    "deep = tf.keras.layers.Dense(\n",
    "    50,\n",
    "    activation='relu',\n",
    "    name='dense_3')(deep)\n",
    "deep = tf.keras.layers.BatchNormalization(name='norm_3')(deep)\n",
    "\n",
    "deep = tf.keras.layers.Dense(\n",
    "    30,\n",
    "    activation='relu',\n",
    "    name='dense_4')(deep)\n",
    "deep = tf.keras.layers.BatchNormalization(name='norm_4')(deep)\n",
    "\n",
    "combined = tf.keras.layers.concatenate([deep, wide_layer_outputs])\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='target')(deep)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[v for v in wide_layer_inputs.values()] + [v for v in deep_layer_inputs.values()],\n",
    "    outputs=output,\n",
    "    name='tipping')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Upajri6FewrM"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adadelta(\n",
    "    lr=1.0,\n",
    "    rho=0.95,\n",
    "    epsilon=None,\n",
    "    decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hqg2u1ZJi51g"
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VV9aD5PZuPeI"
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1P-8lQv2oms"
   },
   "source": [
    "# Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuxR8RXZxK0w"
   },
   "outputs": [],
   "source": [
    "export_path_base = tempfile.mkdtemp(prefix='export')\n",
    "os.environ['BASE_PATH'] = export_path_base\n",
    "\n",
    "export_path = os.path.join(\n",
    "      tf.compat.as_bytes(str(export_path_base)),\n",
    "      tf.compat.as_bytes('tipping/1'))  # model_name/version\n",
    "print('Exporting trained model to', export_path)\n",
    "\n",
    "model.save(export_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9ykBrEJ98O0"
   },
   "source": [
    "# Install and launch TensorFlow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TTKELPX4bhY"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" > /etc/apt/sources.list.d/tensorflow-serving.list\n",
    "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
    "apt-get update\n",
    "apt-get install -y tensorflow-model-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4NXRO0j4cG4"
   },
   "outputs": [],
   "source": [
    "%%script bash --bg\n",
    "tensorflow_model_server \\\n",
    "    --model_name=tipping \\\n",
    "    --model_base_path=\"${BASE_PATH}/tipping\" \\\n",
    "    --rest_api_port=8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQKNLfxZ9_Cg"
   },
   "source": [
    "# Make online predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Jly96Rd3Otw"
   },
   "outputs": [],
   "source": [
    "def convert(obj):\n",
    "    if isinstance(obj, np.int64):\n",
    "        return int(obj)  \n",
    "    raise TypeError\n",
    "\n",
    "for_inference = test.reset_index(drop=True).drop(LABEL_KEY, axis=1).loc[5, :].to_dict()\n",
    "for_inference['pickup_latitude'] = str(for_inference['pickup_latitude'])\n",
    "for_inference['pickup_longitude'] = str(for_inference['pickup_longitude'])\n",
    "for_inference['dropoff_latitude'] = str(for_inference['dropoff_latitude'])\n",
    "for_inference['dropoff_longitude'] = str(for_inference['dropoff_longitude'])\n",
    "\n",
    "data = json.dumps({\n",
    "    'signature_name': 'serving_default',\n",
    "    'instances': [\n",
    "        for_inference\n",
    "    ]\n",
    "}, indent=4, default=convert)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66Rw0w_x3u2A"
   },
   "outputs": [],
   "source": [
    "headers = {'content-type': 'application/json'}\n",
    "json_response = requests.post(\n",
    "    'http://localhost:8501/v1/models/tipping:predict',\n",
    "    data=data,\n",
    "    headers=headers\n",
    ")\n",
    "print(json_response.text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "intro_to_tf2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
